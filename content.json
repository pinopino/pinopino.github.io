{"meta":{"title":"写代码的大雄","subtitle":"工资固然重要，但兴趣也同样重要啊！","description":"写代码的大雄 | 工资固然重要，但兴趣也同样重要啊！","author":"写代码的大雄","url":"https://pinopino.github.io","root":"/"},"pages":[{"title":"categories","date":"2025-03-05T05:42:59.000Z","updated":"2025-03-05T05:43:39.597Z","comments":true,"path":"categories/index.html","permalink":"https://pinopino.github.io/categories/index.html","excerpt":"","text":"layout: categoriescomments: false"},{"title":"tags","date":"2025-03-05T05:42:53.000Z","updated":"2025-03-05T05:43:28.240Z","comments":true,"path":"tags/index.html","permalink":"https://pinopino.github.io/tags/index.html","excerpt":"","text":"layout: tagscomments: false"}],"posts":[{"title":"Asynchronous Programming - Async Performance:Understanding the Costs of Async and Await","slug":"Asynchronous-Programming-Async Performance-Understanding-the-Costs-of-Async","date":"2025-03-05T11:29:05.046Z","updated":"2025-03-05T11:44:17.440Z","comments":true,"path":"2025/03/05/Asynchronous-Programming-Async Performance-Understanding-the-Costs-of-Async/","permalink":"https://pinopino.github.io/2025/03/05/Asynchronous-Programming-Async%20Performance-Understanding-the-Costs-of-Async/","excerpt":"","text":"Asynchronous programming has long been the realm of only the most skilled and masochistic of developers—those with the time, inclination and mental capacity to reason about callback after callback of non-linear control flow. With the Microsoft .NET Framework 4.5, C# and Visual Basic deliver asynchronicity for the rest of us, such that mere mortals can write asynchronous methods almost as easily as writing synchronous methods. No more callbacks. No more explicit marshaling of code from one synchronization context to another. No more worrying about the flowing of results or exceptions. No more tricks that contort existing language features to ease async development. In short, no more hassle. Of course, while it’s now easy to get started writing asynchronous methods (see the articles by Eric Lippert and Mads Torgersen in this issue of MSDN Magazine), doing it really well still requires an understanding of what’s happening under the covers. Any time a language or framework raises the level of abstraction at which a developer can program, it invariably also encapsulates hidden performance costs. In many cases, such costs are negligible and can and should be ignored by the vast number of developers implementing the vast number of scenarios. However, it still behooves more advanced developers to really understand what costs exist so they can take any necessary steps to avoid those costs if they do eventually become visible. Such is the case with the asynchronous methods feature in C# and Visual Basic. In this article, I’ll explore the ins and outs of asynchronous methods, providing you with a solid understanding of how asynchronous methods are implemented under the covers and discussing some of the more nuanced costs involved. Note that this information isn’t meant to encourage you to contort readable code into something that can’t be maintained, all in the name of micro-optimization and performance. It’s simply to give you information that may help you diagnose any problems you may run across, as well as supply a set of tools to help you overcome such potential issues. Note also that this article is based on a preview release of the .NET Framework 4.5, and it’s likely that specific implementation details will change prior to the final release. Getting the Right Mental ModelFor decades, developers have used high-level languages like C#, Visual Basic, F# and C++ to develop efficient applications. This experience has informed those developers about the relevant costs of various operations, and that knowledge has informed best development practices. For example, for most use cases, calling a synchronous method is relatively cheap, even more so when the compiler is able to inline the callee into the call site. Thus, developers learn to refactor code into small, maintainable methods, in general without needing to think about any negative ramifications from the increased method invocation count. These developers have a mental model for what it means to call a method. With the introduction of asynchronous methods, a new mental model is needed. While the C# and Visual Basic languages and compilers are able to provide the illusion of an asynchronous method being just like its synchronous counterpart, under the covers it’s no such thing. The compiler ends up generating a lot of code on behalf of the developer, code akin to the quantities of boilerplate code that developers implementing asynchronicity in days of yore would’ve had to have written and maintained by hand. Further still, the compiler-generated code calls into library code in the .NET Framework, again increasing the work done on behalf of the developer. To get the right mental model, and then to use that mental model to make appropriate development decisions, it’s important to understand what the compiler is generating on your behalf. Think Chunky, Not ChattyWhen working with synchronous code, methods with empty bodies are practically free. This is not the case for asynchronous methods. Consider the following asynchronous method, which has a single statement in its body (and which due to lack of awaits will end up running synchronously): 123public static async Task SimpleBodyAsync() &#123; Console.WriteLine(&quot;Hello, Async World!&quot;);&#125; An intermediate language (IL) decompiler will reveal the true nature of this function once compiled, with output similar to what’s shown in Figure 1. What was a simple one-liner has been expanded into two methods, one of which exists on a helper state machine class. First, there’s a stub method that has the same basic signature as that written by the developer (the method is named the same, it has the same visibility, it accepts the same parameters and it retains its return type), but that stub doesn’t contain any of the code written by the developer. Rather, it contains setup boilerplate. The setup code initializes the state machine used to represent the asynchronous method and then kicks it off using a call to the secondary MoveNext method on the state machine. This state machine type holds state for the asynchronous method, allowing that state to be persisted across asynchronous await points, if necessary. It also contains the body of the method as written by the user, but contorted in a way that allows for results and exceptions to be lifted into the returned Task; for the current position in the method to be maintained so that execution may resume at that location after an await; and so on. Figure 1 Asynchronous Method Boilerplate 1234567891011121314151617181920212223242526272829303132[DebuggerStepThrough] public static Task SimpleBodyAsync() &#123; &lt;SimpleBodyAsync&gt;d__0 d__ = new &lt;SimpleBodyAsync&gt;d__0(); d__.&lt;&gt;t__builder = AsyncTaskMethodBuilder.Create(); d__.MoveNext(); return d__.&lt;&gt;t__builder.Task;&#125; [CompilerGenerated][StructLayout(LayoutKind.Sequential)]private struct &lt;SimpleBodyAsync&gt;d__0 : &lt;&gt;t__IStateMachine &#123; private int &lt;&gt;1__state; public AsyncTaskMethodBuilder &lt;&gt;t__builder; public Action &lt;&gt;t__MoveNextDelegate; public void MoveNext() &#123; try &#123; if (this.&lt;&gt;1__state == -1) return; Console.WriteLine(&quot;Hello, Async World!&quot;); &#125; catch (Exception e) &#123; this.&lt;&gt;1__state = -1; this.&lt;&gt;t__builder.SetException(e); return; &#125; this.&lt;&gt;1__state = -1; this.&lt;&gt;t__builder.SetResult(); &#125; ...&#125; When thinking through what asynchronous methods cost to invoke, keep this boilerplate in mind. The try&#x2F;catch block in the MoveNext method will likely prevent it from getting inlined by the just-in-time (JIT) compiler, so at the very least we’ll now have the cost of a method invocation where in the synchronous case we likely would not (with such a small method body). We have multiple calls into Framework routines (like SetResult). And we have multiple writes to fields on the state machine type. Of course, we need to weigh all of this against the cost of the Console.WriteLine, which will likely dominate all of the other costs involved (it takes locks, it does I&#x2F;O and so forth). Further, notice that there are optimizations the infrastructure does for you. For example, the state machine type is a struct. That struct will only be boxed to the heap if this method ever needs to suspend its execution because it’s awaiting an instance that’s not yet completed, and in this simple method, it never will complete. As such, the boilerplate of this asynchronous method won’t incur any allocations. The compiler and runtime work hard together to minimize the number of allocations involved in the infrastructure. Know When Not to Use AsyncThe .NET Framework attempts to generate efficient asynchronous implementations for asynchronous methods, applying multiple optimizations. However, developers often have domain knowledge than can yield optimizations that would be risky and unwise for the compiler and runtime to apply automatically, given the generality they target. With this in mind, it can actually benefit a developer to avoid using async methods in a certain, small set of use cases, particularly for library methods that will be accessed in a more fine-grained manner. Typically, this is the case when it’s known that the method may actually be able to complete synchronously because the data it’s relying on is already available. When designing asynchronous methods, the Framework developers spent a lot of time optimizing away object allocations. This is because allocations represent one of the largest performance costs possible in the asynchronous method infrastructure. The act of allocating an object is typically quite cheap. Allocating objects is akin to filling your shopping cart with merchandise, in that it doesn’t cost you much effort to put items into your cart; it’s when you actually check out that you need to pull out your wallet and invest significant resources. While allocations are usually cheap, the resulting garbage collection can be a showstopper when it comes to the application’s performance. The act of garbage collection involves scanning through some portion of objects currently allocated and finding those that are no longer referenced. The more objects allocated, the longer it takes to perform this marking. Further, the larger the allocated objects and the more of them that are allocated, the more frequently garbage collection needs to occur. In this manner, then, allocations have a global effect on the system: the more garbage generated by asynchronous methods, the slower the overall program will run, even if micro benchmarks of the asynchronous methods themselves don’t reveal significant costs. For asynchronous methods that actually yield execution (due to awaiting an object that’s not yet completed), the asynchronous method infrastructure needs to allocate a Task object to return from the method, as that Task serves as a unique reference for this particular invocation. However, many asynchronous method invocations can complete without ever yielding. In such cases, the asynchronous method infrastructure may return a cached, already completed Task, one that it can use over and over to avoid allocating unnecessary Tasks. It’s only able to do this in limited circumstances, however, such as when the asynchronous method is a non-generic Task, a Task, or when it’s a Task where TResult is a reference type and the result of the asynchronous method is null. While this set may expand in the future, you can often do better if you have domain knowledge of the operation being implemented. Consider implementing a type like MemoryStream. MemoryStream derives from Stream, and thus can override Stream’s new .NET 4.5 ReadAsync, WriteAsync and FlushAsync methods to provide optimized implementations for the nature of MemoryStream. Because the operation of reading is simply going against an in-memory buffer and is therefore just a memory copy, better performance results if ReadAsync runs synchronously. Implementing this with an asynchronous method would look something like the following: 1234567public override async Task&lt;int&gt; ReadAsync( byte [] buffer, int offset, int count, CancellationToken cancellationToken)&#123; cancellationToken.ThrowIfCancellationRequested(); return this.Read(buffer, offset, count);&#125; Easy enough. And because Read is a synchronous call, and because there are no awaits in this method that will yield control, all invocations of ReadAsync will actually complete synchronously. Now, let’s consider a standard usage pattern of streams, such as a copy operation: 12345byte [] buffer = new byte[0x1000];int numRead;while((numRead = await source.ReadAsync(buffer, 0, buffer.Length)) &gt; 0) &#123; await source.WriteAsync(buffer, 0, numRead);&#125; Notice here that ReadAsync on the source stream for this particular series of calls is always invoked with the same count parameter (the buffer’s length), and thus it’s very likely that the return value (the number of bytes read) will also be repeating. Except in some rare circumstances, it’s very unlikely that the asynchronous method implementation of ReadAsync will be able to use a cached Task for its return value, but you can. Consider rewriting this method as shown in Figure 2. By taking advantage of the specific aspects of this method and its common usage scenarios, we’ve now been able to optimize allocations away on the common path in a way we couldn’t expect the underlying infrastructure to do. With this, every time a call to ReadAsync retrieves the same number of bytes as the previous call to ReadAsync, we’re able to completely avoid any allocation overhead from the ReadAsync method by returning the same Task we returned on the previous invocation. And for a low-level operation like this that we expect to be very fast and to be invoked repeatedly, such an optimization can make a noticeable difference, especially in the number of garbage collections that occur. Figure 2 Optimizing Task Allocations 1234567891011121314151617181920212223private Task&lt;int&gt; m_lastTask; public override Task&lt;int&gt; ReadAsync( byte [] buffer, int offset, int count, CancellationToken cancellationToken)&#123; if (cancellationToken.IsCancellationRequested) &#123; var tcs = new TaskCompletionSource&lt;int&gt;(); tcs.SetCanceled(); return tcs.Task; &#125; try &#123; int numRead = this.Read(buffer, offset, count); return m_lastTask != null &amp;&amp; numRead == m_lastTask.Result ? m_lastTask : (m_lastTask = Task.FromResult(numRead)); &#125; catch(Exception e) &#123; var tcs = new TaskCompletionSource&lt;int&gt;(); tcs.SetException(e); return tcs.Task; &#125;&#125; A related optimization to avoid the task allocation may be done when the scenario dictates caching. Consider a method whose purpose it is to download the contents of a particular Web page and then cache its successfully downloaded contents for future accesses. Such functionality might be written using an asynchronous method as follows (using the new System.Net.Http.dll library in .NET 4.5): 12345678910111213private static ConcurrentDictionary&lt;string,string&gt; s_urlToContents; public static async Task&lt;string&gt; GetContentsAsync(string url)&#123; string contents; if (!s_urlToContents.TryGetValue(url, out contents)) &#123; var response = await new HttpClient().GetAsync(url); contents = response.EnsureSuccessStatusCode().Content.ReadAsString(); s_urlToContents.TryAdd(url, contents); &#125; return contents;&#125; This is a straightforward implementation. And for calls to GetContentsAsync that can’t be satisfied from the cache, the overhead of constructing a new Task to represent this download will be negligible when compared to the network-related costs. However, for cases where the contents may be satisfied from the cache, it could represent a non-negligible cost, an object allocation simply to wrap and hand back already available data. To avoid that cost (if doing so is required to meet your performance goals), you could rewrite this method as shown in Figure 3. We now have two methods: a synchronous public method, and an asynchronous private method to which the public method delegates. The dictionary is now caching the generated tasks rather than their contents, so future attempts to download a page that’s already been successfully downloaded can be satisfied with a simple dictionary access to return an already existing task. Internally, we also take advantage of the ContinueWith methods on Task that allow us to store the task into the dictionary once the Task has completed—but only if the download succeeded. Of course, this code is more complicated and requires more thought to write and maintain, so as with any performance optimizations, avoid spending time making them until performance testing proves that the complications make an impactful and necessary difference. Whether such optimizations make a difference really depends on usage scenarios. You’ll want to come up with a suite of tests that represent common usage patterns, and use analysis of those tests to determine whether these complications improve your code’s performance in a meaningful way. Figure 3 Manually Caching Tasks 1234567891011121314151617181920private static ConcurrentDictionary&lt;string,Task&lt;string&gt;&gt; s_urlToContents; public static Task&lt;string&gt; GetContentsAsync(string url) &#123; Task&lt;string&gt; contents; if (!s_urlToContents.TryGetValue(url, out contents)) &#123; contents = GetContentsInternalAsync(url); contents.ContinueWith(delegate &#123; s_urlToContents.TryAdd(url, contents); &#125;, CancellationToken.None, TaskContinuationOptions.OnlyOnRanToCompletion | TaskContinuatOptions.ExecuteSynchronously, TaskScheduler.Default); &#125; return contents;&#125;private static async Task&lt;string&gt; GetContentsInternalAsync(string url) &#123; var response = await new HttpClient().GetAsync(url); return response.EnsureSuccessStatusCode().Content.ReadAsString();&#125; Another task-related optimization to consider is whether you even need the returned Task from an asynchronous method. C# and Visual Basic both support the creation of asynchronous methods that return void, in which case no Task is allocated for the method, ever. Asynchronous methods exposed publicly from libraries should always be written to return a Task or Task, because you as a library developer don’t know whether the consumer desires to wait on the completion of that method. However, for certain internal usage scenarios, void-returning asynchronous methods can have their place. The primary reason void-returning asynchronous methods exist is to support existing event-driven environments, like ASP.NET and Windows Presentation Foundation (WPF). They make it easy to implement button handlers, page-load events and the like through the use of async and await. If you do consider using an async void method, be very careful around exception handling: exceptions that escape an async void method bubble out into whatever SynchronizationContext was current at the time the async void method was invoked. Care About ContextThere are many kinds of “context” in the .NET Framework: LogicalCallContext, SynchronizationContext, HostExecutionContext, SecurityContext, ExecutionContext and more (from the sheer number you might expect that the developers of the Framework are monetarily incentivized to introduce new contexts, but I assure you we’re not). Some of these contexts are very relevant to asynchronous methods, not only in functionality, but also in their impact on asynchronous method performance. SynchronizationContextSynchronizationContext plays a big role in asynchronous methods. A “synchronization context” is simply an abstraction over the ability to marshal delegate invocation in a manner specific to a given library or framework. For example, WPF provides a DispatcherSynchronizationContext to represent the UI thread for a Dispatcher: posting a delegate to this synchronization context causes that delegate to be queued for execution by the Dispatcher on its thread. ASP.NET provides an AspNetSynchronizationContext, which is used to ensure that asynchronous operations that occur as part of the processing of an ASP.NET request are executed serially and are associated with the right HttpContext state. And so on. All told, there are around 10 concrete implementations of SynchronizationContext within the .NET Framework, some public, some internal. When awaiting Tasks and other awaitable types provided by the .NET Framework, the “awaiters” for those types (like TaskAwaiter) capture the current SynchronizationContext at the time the await is issued. Upon completion of the awaitable, if there was a current SynchronizationContext that got captured, the continuation representing the remainder of the asynchronous method is posted to that SynchronizationContext. With that, developers writing an asynchronous method called from a UI thread don’t need to manually marshal invocations back to the UI thread in order to modify UI controls: such marshaling is handled automatically by the Framework infrastructure. Unfortunately, such marshaling also involves cost. For application developers using await to implement their control flow, this automatic marshaling is almost always the right solution. Libraries, however, are often a different story. Application developers typically need such marshaling because their code cares about the context under which it’s running, such as being able to access UI controls, or being able to access the HttpContext for the right ASP.NET request. Most libraries, however, do not suffer this constraint. As a result, this automatic marshaling is frequently an entirely unnecessary cost. Consider again the code shown earlier to copy data from one stream to another: 12345byte [] buffer = new byte[0x1000];int numRead;while((numRead = await source.ReadAsync(buffer, 0, buffer.Length)) &gt; 0) &#123; await source.WriteAsync(buffer, 0, numRead);&#125; If this copy operation is invoked from a UI thread, every awaited read and write operation will force the completion back to the UI thread. For a megabyte of source data and Streams that complete reads and writes asynchronously (which is most of them), that means upward of 500 hops from background threads to the UI thread. To address this, the Task and Task types provide a ConfigureAwait method. ConfigureAwait accepts a Boolean continueOnCapturedContext parameter that controls this marshaling behavior. If the default of true is used, the await will automatically complete back on the captured SynchronizationContext. If false is used, however, the SynchronizationContext will be ignored and the Framework will attempt to continue the execution wherever the previous asynchronous operation completed. Incorporating this into the stream-copying code results in the following more efficient version: 123456byte [] buffer = new byte[0x1000];int numRead;while((numRead = await source.ReadAsync(buffer, 0, buffer.Length).ConfigureAwait(false)) &gt; 0) &#123; await source.WriteAsync(buffer, 0, numRead).ConfigureAwait(false);&#125; For library developers, this performance impact alone is sufficient to warrant always using ConfigureAwait, unless it’s the rare circumstance where the library has domain knowledge of its environment and does need to execute the body of the method with access to the correct context. There’s another reason, beyond performance, to use ConfigureAwait in library code. Suppose the preceding code, without ConfigureAwait, was in a method called CopyStreamToStreamAsync, which was invoked from a WPF UI thread, like so: 12345private void button1_Click(object sender, EventArgs args) &#123; Stream src = …, dst = …; Task t = CopyStreamToStreamAsync(src, dst); t.Wait(); // deadlock!&#125; Here, the developer should have written button1_Click as an async method and then await-ed the Task instead of using its synchronous Wait method. The Wait method has its important uses, but it’s almost always wrong to use it for waiting in a UI thread like this. The Wait method won’t return until the Task has completed. In the case of CopyStreamToStreamAsync, the contained awaits try to Post back to the captured SynchronizationContext, and the method can’t complete until those Posts complete (because the Posts are used to process the remainder of the method). But those Posts won’t complete, because the UI thread that would process them is blocked in the call to Wait. This is a circular dependency, resulting in a deadlock. If CopyStreamToStreamAsync had instead been written using ConfigureAwait(false), there would be no circular dependency and no deadlock. ExecutionContextExecutionContext is an integral part of the .NET Framework, yet most developers are blissfully unaware of its existence. ExecutionContext is the granddaddy of contexts, encapsulating multiple other contexts like SecurityContext and LogicalCallContext, and representing everything that should be automatically flowed across asynchronous points in code. Any time you’ve used ThreadPool.QueueUserWorkItem, Task.Run, Delegate.BeginInvoke, Stream.BeginRead, WebClient.DownloadStringAsync or any other asynchronous operation in the Framework, under the covers ExecutionContext was captured if possible (via ExecutionContext.Capture), and that captured context was then used to process the provided delegate (via ExecutionContext.Run). For example, if the code invoking ThreadPool.QueueUserWorkItem was impersonating a Windows identity at the time, that same Windows identity would be impersonated in order to run the supplied WaitCallback delegate. And if the code invoking Task.Run had first stored data into the LogicalCallContext, that same data would be accessible through the LogicalCallContext within the supplied Action delegate. ExecutionContext is also flowed across awaits on tasks. There are multiple optimizations in place in the Framework to avoid capturing and running under a captured ExecutionContext when doing so is unnecessary, as doing so can be quite expensive. However, actions like impersonating a Windows identity or storing data into LogicalCallContext will thwart these optimizations. Avoiding operations that manipulate ExecutionContext, such as WindowsIdentity.Impersonate and CallContext.LogicalSetData, results in better performance when using asynchronous methods, and when using asynchrony in general. Lift Your Way out of Garbage CollectionAsynchronous methods provide a nice illusion when it comes to local variables. In a synchronous method, local variables in C# and Visual Basic are stack-based, such that no heap allocations are necessary to store those locals. However, in asynchronous methods, the stack for the method goes away when the asynchronous method is suspending at an await point. For data to be available to the method after an await resumes, that data must be stored somewhere. Thus, the C# and Visual Basic compilers “lift” locals into a state machine struct, which is then boxed to the heap at the first await that suspends so that locals may survive across await points. Earlier in this article, I discussed how the cost and frequency of garbage collection is influenced by the number of objects allocated, while the frequency of garbage collection is also influenced by the size of objects allocated. The bigger the objects being allocated, the more often garbage collection will need to run. Thus, in an asynchronous method, the more locals that need to be lifted to the heap, the more often garbage collections will occur. As of the time of this writing, the C# and Visual Basic compilers sometimes lift more than is truly necessary. For example, consider the following code snippet: 123456public static async Task FooAsync() &#123; var dto = DateTimeOffset.Now; var dt = dto.DateTime; await Task.Yield(); Console.WriteLine(dt);&#125; The dto variable isn’t read at all after the await point, and thus the value written to it before the await doesn’t need to survive across the await. However, the state machine type generated by the compiler to store locals still contains the dto reference, as shown in Figure 4. Figure 4 Local Lifting 123456789101112131415[StructLayout(LayoutKind.Sequential), CompilerGenerated]private struct &lt;FooAsync&gt;d__0 : &lt;&gt;t__IStateMachine &#123; private int &lt;&gt;1__state; public AsyncTaskMethodBuilder &lt;&gt;t__builder; public Action &lt;&gt;t__MoveNextDelegate; public DateTimeOffset &lt;dto&gt;5__1; public DateTime &lt;dt&gt;5__2; private object &lt;&gt;t__stack; private object &lt;&gt;t__awaiter; public void MoveNext(); [DebuggerHidden] public void &lt;&gt;t__SetMoveNextDelegate(Action param0);&#125; This slightly bloats the size of that heap object beyond what’s truly necessary. If you find that garbage collections are occurring more frequently than you expect, take a look at whether you really need all of the temporary variables you’ve coded into your asynchronous method. This example could be rewritten as follows to avoid the extra field on the state machine class: 12345public static async Task FooAsync() &#123; var dt = DateTimeOffset.Now.DateTime; await Task.Yield(); Console.WriteLine(dt);&#125; Moreover, the .NET garbage collector (GC) is a generational collector, meaning that it partitions the set of objects into groups, known as generations: at a high-level, new objects are allocated in generation 0, and then all objects that survive a collection are promoted up a generation (the .NET GC currently uses generations 0, 1 and 2). This enables faster collections by allowing the GC to frequently collect only from a subset of the known object space. It’s based on the philosophy that objects newly allocated will also go away quickly, while objects that have been around for a long time will continue to be around for a long time. What this means is that if an object survives generation 0, it will likely end up being around for a while, continuing to put pressure on the system for that additional time. And that means we really want to ensure that objects are made available to garbage collection as soon as they’re no longer needed. With the aforementioned lifting, locals get promoted to fields of a class that stays rooted for the duration of the asynchronous method’s execution (as long as the awaited object properly maintains a reference to the delegate to invoke upon completion of the awaited operation). In synchronous methods, the JIT compiler is able to keep track of when locals will never again be accessed, and at such points can help the GC to ignore those variables as roots, thus making the referenced objects available for collection if they’re not referenced anywhere else. However, in asynchronous methods, these locals remain referenced, which means the objects they reference may survive much longer than if these had been real locals. If you find that objects are remaining alive well past their use, consider nulling out the locals referencing those objects when you’re done with them. Again, this should be done only if you find that it’s actually the cause of a performance problem, as it otherwise complicates the code unnecessarily. Furthermore, the C# and Visual Basic compilers could be updated by final release or otherwise in the future to handle more of these scenarios on the developer’s behalf, so any such code written today is likely to become obsolete in the future. Avoid ComplexityThe C# and Visual Basic compilers are fairly impressive in terms of where you’re allowed to use awaits: almost anywhere. Await expressions may be used as part of larger expressions, allowing you to await Task instances in places you might have any other value-returning expression. For example, consider the following code, which returns the sum of three tasks’ results: 12345678910public static async Task&lt;int&gt; SumAsync( Task&lt;int&gt; a, Task&lt;int&gt; b, Task&lt;int&gt; c)&#123; return Sum(await a, await b, await c);&#125; private static int Sum(int a, int b, int c)&#123; return a + b + c;&#125; The C# compiler allows you to use the expression “await b” as an argument to the Sum function. However, there are multiple awaits here whose results are passed as parameters to Sum, and due to order of evaluation rules and how async is implemented in the compiler, this particular example requires the compiler to “spill” the temporary results of the first two awaits. As you saw previously, locals are preserved across await points by having them lifted into fields on the state machine class. However, for cases like this one, where the values are on the CLR evaluation stack, those values aren’t lifted into the state machine but are instead spilled to a single temporary object and then referenced by the state machine. When you complete the await on the first task and go to await the second one, the compiler generates code that boxes the first result and stores the boxed object into a single &lt;&gt;t__stack field on the state machine. When you complete the await on the second task and go to await the third one, the compiler generates code that creates a Tuple&lt;int,int&gt; from the first two values, storing that tuple into the same &lt;&gt;__stack field. This all means that, depending on how you write your code, you could end up with very different allocation patterns. Consider instead writing SumAsync as follows: 12345678public static async Task&lt;int&gt; SumAsync( Task&lt;int&gt; a, Task&lt;int&gt; b, Task&lt;int&gt; c)&#123; int ra = await a; int rb = await b; int rc = await c; return Sum(ra, rb, rc);&#125; With this change, the compiler will now emit three more fields onto the state machine class to store ra, rb and rc, and no spilling will occur. Thus, you have a trade-off: a larger state machine class with fewer allocations, or a smaller state machine class with more allocations. The total amount of memory allocated will be larger in the spilling case, as each object allocated has its own memory overhead, but in the end performance testing could reveal that’s still better. In general, as mentioned previously, you shouldn’t think through these kinds of micro-optimizations unless you find that the allocations are actually the cause of grief, but regardless, it’s helpful to know where these allocations are coming from. Of course, there’s arguably a much larger cost in the preceding examples that you should be aware of and proactively consider. The code isn’t able to invoke Sum until all three awaits have completed, and no work is done in between the awaits. Each of these awaits that yields requires a fair amount of work, so the fewer awaits you need to process, the better. It would behoove you, then, to combine all three of these awaits into just one by waiting on all of the tasks at once with Task.WhenAll: 123456public static async Task&lt;int&gt; SumAsync( Task&lt;int&gt; a, Task&lt;int&gt; b, Task&lt;int&gt; c)&#123; int [] results = await Task.WhenAll(a, b, c); return Sum(results[0], results[1], results[2]);&#125; The Task.WhenAll method here returns a Task&lt;TResult[]&gt; that won’t complete until all of the supplied tasks have completed, and it does so much more efficiently than just waiting on each individual task. It also gathers up the result from each task and stores it into an array. If you want to avoid that array, you can do that by forcing binding to the non-generic WhenAll method that works with Task instead of Task. For ultimate performance, you could also take a hybrid approach, where you first check to see if all of the tasks have completed successfully, and if they have, get their resultsindividually—but if they haven’t, then await a WhenAll of those that haven’t. That will avoid any allocations involved in the call to WhenAll when it’s unnecessary, such as allocating the params array to be passed into the method. And, as previously mentioned, we’d want this library function to also suppress context marshaling. Such a solution is shown in Figure 5. Figure 5 Applying Multiple Optimizations 12345678910111213141516public static Task&lt;int&gt; SumAsync( Task&lt;int&gt; a, Task&lt;int&gt; b, Task&lt;int&gt; c)&#123; return (a.Status == TaskStatus.RanToCompletion &amp;&amp; b.Status == TaskStatus.RanToCompletion &amp;&amp; c.Status == TaskStatus.RanToCompletion) ? Task.FromResult(Sum(a.Result, b.Result, c.Result)) : SumAsyncInternal(a, b, c);&#125; private static async Task&lt;int&gt; SumAsyncInternal( Task&lt;int&gt; a, Task&lt;int&gt; b, Task&lt;int&gt; c)&#123; await Task.WhenAll((Task)a, b, c).ConfigureAwait(false); return Sum(a.Result, b.Result, c.Result);&#125; Asynchronicity and PerformanceAsynchronous methods are a powerful productivity tool, enabling you to more easily write scalable and responsive libraries and applications. It’s important to keep in mind, though, that asynchronicity is not a performance optimization for an individual operation. Taking a synchronous operation and making it asynchronous will invariably degrade the performance of that one operation, as it still needs to accomplish everything that the synchronous operation did, but now with additional constraints and considerations. A reason you care about asynchronicity, then, is performance in the aggregate: how your overall system performs when you write everything asynchronously, such that you can overlap I&#x2F;O and achieve better system utilization by consuming valuable resources only when they’re actually needed for execution. The asynchronous method implementation provided by the .NET Framework is well-optimized, and often ends up providing as good or better performance than well-written asynchronous implementations using existing patterns and volumes more code. Any time you’re planning to develop asynchronous code in the .NET Framework from now on, asynchronous methods should be your tool of choice. Still, it’s good for you as a developer to be aware of everything the Framework is doing on your behalf in these asynchronous methods, so you can ensure the end result is as good as it can possibly be.","categories":[],"tags":[{"name":".NET","slug":"NET","permalink":"https://pinopino.github.io/tags/NET/"},{"name":"异步","slug":"异步","permalink":"https://pinopino.github.io/tags/%E5%BC%82%E6%AD%A5/"},{"name":"performance","slug":"performance","permalink":"https://pinopino.github.io/tags/performance/"}]},{"title":"Little-known gems:Atomic conditional removals from ConcurrentDictionary","slug":"Little-known-gems-Atomic-conditional-removals-from-ConcurrentDictionary","date":"2025-03-05T11:08:32.342Z","updated":"2025-03-05T11:43:14.358Z","comments":true,"path":"2025/03/05/Little-known-gems-Atomic-conditional-removals-from-ConcurrentDictionary/","permalink":"https://pinopino.github.io/2025/03/05/Little-known-gems-Atomic-conditional-removals-from-ConcurrentDictionary/","excerpt":"","text":"ConcurrentDictionary&lt;TKey,TValue&gt;, first introduced in .NET 4, is an efficient dictionary data structure that enables thread-safe reading and writing, meaning that multiple threads may all be accessing the dictionary at the same time without corrupting it. It supports adding through its TryAdd method, conditional updates through its TryUpdate method, non-conditional adds or updates through its indexer’s setter, and removals through its TryRemove method, through which a key&#x2F;value pair is removed if the user-provided key matches. It also has several compound methods, such as GetOrAdd and AddOrUpdate. Lately, however, we’ve seen several folks ask for further support on ConcurrentDictionary, that of removing a key&#x2F;value pair only if both the user-provided key and value match the corresponding pair currently stored in the dictionary. You want it, you got it! I mean, literally, you already have it. As with Dictionary&lt;TKey,TValue&gt;, ConcurrentDictionary&lt;TKey,TValue&gt; implements ICollection&lt;KeyValuePair&lt;TKey,TValue&gt;&gt;. ICollection exposes a Remove method, so ICollection&lt;KeyValuePair&lt;TKey,TValue&gt;&gt; has a Remove(KeyValuePair&lt;TKey,TValue&gt;) method. Dictionary&lt;TKey,TValue&gt; implements Remove such that it only removes the element if both the key and the value match the data in the dictionary, and thus ConcurrentDictionary&lt;TKey,TValue&gt; does the same. And as it’s a concurrent data structure, ConcurrentDictionary&lt;TKey,TValue&gt; ensures that the comparison is done atomically with the removal. Tada! Of course, just as with Dictionary&lt;TKey,TValue&gt;, ConcurrentDictionary&lt;TKey,TValue&gt; implements ICollection&lt;KeyValuePair&lt;TKey,TValue&gt;&gt; explicitly, so if you want access to this method, you’ll need to first cast the dictionary to the interface. Here’s an extension method to help you with that cause: 1234567public static bool TryRemove&lt;TKey, TValue&gt;( this ConcurrentDictionary&lt;TKey, TValue&gt; dictionary, TKey key, TValue value)&#123; if (dictionary == null) throw new ArgumentNullException(“dictionary”); return ((ICollection&lt;KeyValuePair&lt;TKey, TValue&gt;&gt;)dictionary).Remove( new KeyValuePair&lt;TKey, TValue&gt;(key, value));&#125; Enjoy!","categories":[],"tags":[{"name":".NET","slug":"NET","permalink":"https://pinopino.github.io/tags/NET/"}]},{"title":"await anything","slug":"await-anything","date":"2025-03-05T11:01:47.268Z","updated":"2025-03-05T11:22:43.499Z","comments":true,"path":"2025/03/05/await-anything/","permalink":"https://pinopino.github.io/2025/03/05/await-anything/","excerpt":"","text":"One of the very cool things about the new await keyword in C# and Visual Basic is that it’s pattern based. It works great with Task and Task, and awaiting those two types will represent the vast majority of uses, but they’re by no means the only types that can be awaited. The languages support awaiting any instance that exposes the right method (either instance method or extension method): GetAwaiter. A GetAwaiter needs to implement the INotifyCompletion interface (and optionally the ICriticalNotifyCompletion interface) and return a type that itself exposes three members: 123bool IsCompleted &#123; get; &#125;void OnCompleted(Action continuation);TResult GetResult(); // TResult can also be void As an example of this, Task’s GetAwaiter method returns a value of type TaskAwaiter: 1public struct TaskAwaiter : ICriticalNotifyCompletion and that’s what enables awaiting the Task. This is a simplification, but in short the OnCompleted method registers the Action as a continuation onto the Task (e.g. with ContinueWith), such that when the task completes, it will cause the compiler-generated state machine around the await to pick back up where it left off. The title of this post is “await anything;”, so let’s see how we can await things besides Task and Task. To do that, we’ll need appropriate “awaiter” types for the “awaitable” type to await. That doesn’t mean we have to write new “awaiter” types, however. There are really two different approaches to making something awaitable: develop a new awaiter type that exposes the right pattern, or figure out how to create a Task or Task from the thing being awaited, and then just reuse Task or Task’s awaiter. For the majority of cases, the latter approach is very straightforward, so we’ll start with that. Let’s say you want to be able to write code like: 1await TimeSpan.FromMinutes(15); in order to asynchronously pause for 15 minutes. To do that, we can develop a 1-line GetAwaiter method for TimeSpan: 1234public static TaskAwaiter GetAwaiter(this TimeSpan timeSpan)&#123; return TaskEx.Delay(timeSpan).GetAwaiter();&#125; That’s it. Or let’s say we like waiting for periods of time so much, that we want to simply this down to just: 1await 15000; // in milliseconds No problem, we can do that with another one-line awaiter: 1234public static TaskAwaiter GetAwaiter(this Int32 millisecondsDue)&#123; return TimeSpan.FromMilliseconds(millisecondsDue).GetAwaiter();&#125; Let’s say we like waiting for time-like things so much that we want to be able to wait until a particular date&#x2F;time, ala 1await DateTimeOffset.UtcNow.AddMinutes(1); Again, piece of cake: 1234public static TaskAwaiter GetAwaiter(this DateTimeOffset dateTimeOffset)&#123; return (dateTimeOffset – DateTimeOffset.UtcNow).GetAwaiter();&#125; Tired of time? Alright. The GetAwaiter function for Task allows you to wait for a single task, how about enabling waiting for an enumerable of tasks so that you can write code like: 1await from url in urls select DownloadAsync(url); Easy peasy: 1234public static TaskAwaiter GetAwaiter(this IEnumerable&lt;Task&gt; tasks)&#123; return TaskEx.WhenAll(tasks).GetAwaiter();&#125; All of the examples thus far were one-liners because we already have a function that takes the input to the extension method and produces a task from it. However, with just a few more lines, you can convert almost anything that has some notion of future completion into a task, through the TaskCompletionSource type. If you can express your need by completing the statement “I want to await until …” or “I want the await to complete when …”, this is likely a good approach for you. As an example, consider wanting to spin up another process and then asynchronously wait for that process to complete, e.g. 1await Process.Start(“Foo.exe”); You could do that with a GetAwaiter method like the following: 12345678public static TaskAwaiter&lt;int&gt; GetAwaiter(this Process process)&#123; var tcs = new TaskCompletionSource&lt;int&gt;(); process.EnableRaisingEvents = true; process.Exited += (s, e) =&gt; tcs.TrySetResult(process.ExitCode); if (process.HasExited) tcs.TrySetResult(process.ExitCode); return tcs.Task.GetAwaiter();&#125; Or maybe you want to asynchronously wait until cancellation is requested, e.g. 1await cancellationToken; That could be done with a GetAwaiter like the following: 12345678public static TaskAwaiter GetAwaiter(this CancellationToken cancellationToken)&#123; var tcs = new TaskCompletionSource&lt;bool&gt;(); Task t = tcs.Task; if (cancellationToken.IsCancellationRequested) tcs.SetResult(true); else cancellationToken.Register(s =&gt; ((TaskCompletionSource&lt;bool&gt;)s).SetResult(true), tcs); return t.GetAwaiter();&#125; You get the idea. The second approach to making an awaitable type is to implement a custom awaiter. This could either be a separate type that’s returned by GetAwaiter and that exposes the IsCompleted&#x2F;OnCompleted&#x2F;GetResult members, or it could be a GetAwaiter method that returns “this”, with IsCompleted&#x2F;OnCompleted&#x2F;GetResult also exposed on the awaitable type. You’d typically go this route if you can’t express your desire as “I want the await to complete when…”, but rather as “When the await completes, I want to continue executing …”, filling in the blank for that “…”. In particular, you’d need to use this approach if you need full control over how (rather than when) the “Action continuation” delegate is invoked. Imagine, for example, that you wanted to launch some work to run on the ThreadPool. This work would compute a string and then store the result into a control on your UI. To modify the control, you need to be on the UI thread, so you somehow need to transition to the UI thread to do that work. If this were, for example, a Windows Forms application, we could accomplish this by building an awaiter for a Windows Forms Control. That would allow us to write code like: 123456ThreadPool.QueueUserWorkItem(async delegate&#123; string text = ComputeString(); await button1; button1.Text = text;&#125;); We want the operation of awaiting the button1 to transition to the UI thread and then continue the execution there. We can do that with an implementation like the following: 1234567891011121314151617181920212223242526public static ControlAwaiter GetAwaiter(this Control control)&#123; return new ControlAwaiter(control);&#125;public struct ControlAwaiter : INotifyCompletion&#123; private readonly Control m_control; public ControlAwaiter(Control control) &#123; m_control = control; &#125; public bool IsCompleted &#123; get &#123; return !m_control.InvokeRequired; &#125; &#125; public void OnCompleted(Action continuation) &#123; m_control.BeginInvoke(continuation); &#125; public void GetResult() &#123; &#125;&#125; You can also combine these approaches, such as by writing a custom awaiter which wraps the awaiter for a task, layering on additional functionality. For example, culture information is not flowed by default as part of ExecutionContext, which is the standard .NET mechanism for transferring important environmental information across asynchronous invocations. What if we wanted to make it easy to flow culture? Imagine the following syntax for awaiting a task with the flow of culture: 1await task.WithCulture(); We could enable that with code like the following: 123456789101112131415161718192021222324252627282930313233public static CultureAwaiter WithCurrentCulture(this Task task)&#123; return new CultureAwaiter(task);&#125;public class CultureAwaiter : INotifyCompletion&#123; private readonly TaskAwaiter m_awaiter; private CultureInfo m_culture; public CultureAwaiter(Task task) &#123; if (task == null) throw new ArgumentNullException(“task”); m_awaiter = task.GetAwaiter(); &#125; public CultureAwaiter GetAwaiter() &#123; return this; &#125; public bool IsCompleted &#123; get &#123; return m_awaiter.IsCompleted; &#125; &#125; public void OnCompleted(Action continuation) &#123; m_culture = Thread.CurrentThread.CurentCulture; m_awaiter.OnCompleted(continuation); &#125; public void GetResult() &#123; if (m_culture != null) Thread.CurrentThread.CurrentCulture = m_culture; m_awaiter.GetResult(); &#125;&#125; This awaiter implementation wraps a TaskAwaiter, and this implementation’s IsCompleted, OnCompleted, and GetResult members delegate to the contained TaskAwaiter’s. On top of that, though, the implementation captures the current culture in OnCompleted and then restores it in GetResult. By now, it should be obvious that there are loads of interesting possibilities here. I look forward to seeing all the interesting and useful awaiters you come up with. Just keep in mind that while there are plenty of “cool” things you can do, code readability and maintainability is really important, so make sure that the coolness isn’t trumped by lack of clarity about the code’s meaning.","categories":[],"tags":[{"name":".NET","slug":"NET","permalink":"https://pinopino.github.io/tags/NET/"},{"name":"珠玑","slug":"珠玑","permalink":"https://pinopino.github.io/tags/%E7%8F%A0%E7%8E%91/"},{"name":"异步","slug":"异步","permalink":"https://pinopino.github.io/tags/%E5%BC%82%E6%AD%A5/"}]},{"title":"Await, and UI, and deadlocks! Oh my!","slug":"Await-and-UI-and-deadlocks-Oh-my","date":"2025-03-05T03:37:09.375Z","updated":"2025-03-05T11:14:17.407Z","comments":true,"path":"2025/03/05/Await-and-UI-and-deadlocks-Oh-my/","permalink":"https://pinopino.github.io/2025/03/05/Await-and-UI-and-deadlocks-Oh-my/","excerpt":"","text":"It’s been awesome seeing the level of interest developers have had for the Async CTP and how much usage it’s getting. Of course, with any new technology there are bound to be some hiccups. One issue I’ve seen arise now multiple times is developers accidentally deadlocking their application by blocking their UI thread, so I thought it would be worthwhile to take a few moments to explore the common cause of this and how to avoid such predicaments. At its core, the new async language functionality aims to restore the ability for developers to write the sequential, imperative code they’re used to writing, but to have it be asynchronous in nature rather than synchronous. That means that when operations would otherwise tie up the current thread of execution, they’re instead offloaded elsewhere, allowing the current thread to make forward progress and do other useful work while, in effect, asynchronously waiting for the spawned operation to complete. In both server and client applications, this can be crucial for application scalability, and in client applications in particular it’s also really useful for responsiveness. Most UI frameworks, such as Windows Forms and WPF, utilize a message loop to receive and process incoming messages. These messages include things like notifications of keys being typed on a keyboard, or buttons being clicked on a mouse, or controls in the user interface being manipulated, or the need to refresh an area of the window, or even the application sending itself a message dictating some code to be executed. In response to these messages, the UI performs some action, such as redrawing its surface, or changing the text being displayed, or adding items to one of its controls., or running the code that was posted to it. The “message loop” is typically literally a loop in code, where a thread continually waits for the next message to arrive, processes it, goes back to get the next message, processes it, and so on. As long as that thread is able to quickly process messages as soon as they arrive, the application remains responsive, and the application’s users remain happy. If, however, processing a particular message takes too long, the thread running the message loop code will be unable to pick up the next message in a timely fashion, and responsiveness will decrease. This could take the form of pauses in responding to user input, and if the thread’s delays get bad enough (e.g. an infinite delay), the application “hanging”. In a framework like Windows Forms or WPF, when a user clicks a button, that typically ends up sending a message to the message loop, which translates the message into a call to a handler of some kind, such as a method on the class representing the user interface, e.g.: 12345private void button1_Click(object sender, RoutedEventArgs e)&#123; string s = LoadString(); textBox1.Text = s;&#125; Here, when I click the button1 control, the message will inform WPF to invoke the button1_Click method, which will in turn run a method LoadString to get a string value, and store that string value into the textBox1 control’s Text property. As long as LoadString is quick to execute, all is well, but the longer LoadString takes, the more time the UI thread is delayed inside button1_Click, unable to return to the message loop to pick up and process the next message. To address that, we can choose to load the string asynchronously, meaning that rather than blocking the thread calling button1_Click from returning to the message loop until the string loading has completed, we’ll instead just have that thread launch the loading operation and then go back to the message loop. Only when the loading operation completes will we then send another message to the message loop to say “hey, that loading operation you previously started is done, and you can pick up where you left off and continue executing.” Imagine we had a method: 1public Task&lt;string&gt; LoadStringAsync(); This method will return very quickly to its caller, handing back a .NET Task object that represents the future completion of the asynchronous operation and its future result. At some point in the future when the operation completes, the task object will be able to hand out the operations’ result, which could be the string in the case of successful loading, or an exception in the case of failure. Either way, the task object provides several mechanisms to notify the holder of the object that the loading operation has completed. One way is to synchronously block waiting for the task to complete, and that can be accomplished by calling the task’s Wait method, or by accessing its Result, which will implicitly wait until the operation has completed… in both of these cases, a call to these members will not complete until the operation has completed. An alternative way is to receive an asynchronous callback, where you register with the task a delegate that will be invoked when the task completes. That can be accomplished using one of the Task’s ContinueWith methods. With ContinueWith, we can now rewrite our previous button1_Click method to not block the UI thread while we’re asynchronously waiting for the loading operation to complete: 12345private void button1_Click(object sender, RoutedEventArgs e)&#123; Task&lt;string&gt; s = LoadStringAsync(); s.ContinueWith(delegate &#123; textBox1.Text = s.Result; &#125;); // warning: buggy&#125; This does in fact asynchronously launch the loading operation, and then asynchronously run the code to store the result into the UI when the operation completes. However, we now have a new problem. UI frameworks like Windows Forms, WPF, and Silverlight all place a restriction on which threads are able to access UI controls, namely that the control can only be accessed from the thread that created it. Here, however, we’re running the callback to update the Text of textBox1on some arbitrary thread, wherever the Task Parallel Library (TPL) implementation of ContinueWith happened to put it. To address this, we need some way to get back to the UI thread. Different UI frameworks provide different mechanisms for doing this, but in .NET they all take basically the same shape, a BeginInvoke method you can use to pass some code as a message to the UI thread to be processed: 1234567891011private void button1_Click(object sender, RoutedEventArgs e)&#123; Task&lt;string&gt; s = LoadStringAsync(); s.ContinueWith(delegate &#123; Dispatcher.BeginInvoke(new Action(delegate &#123; textBox1.Text = s.Result; &#125;)); &#125;);&#125; The .NET Framework further abstracts over these mechanisms for getting back to the UI thread, and in general a mechanism for posting some code to a particular context, through the SynchronizationContext class. A framework can establish a current context, available through the SynchronizationContext.Current property, which provides a SynchronizationContext instance representing the current environment. This instance’s Post method will marshal a delegate back to this environment to be invoked: in a WPF app, that means bringing you back to the dispatcher, or UI thread, you were previously on. So, we can rewrite the previous code as follows: 123456789private void button1_Click(object sender, RoutedEventArgs e)&#123; var sc = SynchronizationContext.Current; Task&lt;string&gt; s = LoadStringAsync(); s.ContinueWith(delegate &#123; sc.Post(delegate &#123; textBox1.Text = s.Result; &#125;, null); &#125;);&#125; and in fact this pattern is so common, TPL in .NET 4 provides the TaskScheduler.FromCurrentSynchronizationContext() method, which allows you to do the same thing with code like: 12345private void button1_Click(object sender, RoutedEventArgs e)&#123; LoadStringAsync().ContinueWith(s =&gt; textBox1.Text = s.Result, TaskScheduler.FromCurrentSynchronizationContext());&#125; As mentioned, this works by “posting” the delegate back to the UI thread to be executed. That posting is a message like any other, and it requires the UI thread to go through its message loop, pick up the message, and process it (which will result in invoking the posted delegate). In order for the delegate to be invoked then, the thread first needs to return to the message loop, which means it must leave the button1_Click method. Now, there’s still a fair amount of boilerplate code to write above, and it gets orders of magnitude worse when you start introducing more complicated flow control constructs, like conditionals and loops. To address this, the new async language feature allows you to write this same code as: 12345private async void button1_Click(object sender, RoutedEventArgs e)&#123; string s = await LoadStringAsync(); textBox1.Text = s;&#125; For all intents and purposes, this is the same as the previous code shown, and you can see how much cleaner it is… in fact, it’s close to identical in the code required to our original synchronous implementation. But, of course, this one is asynchronous: after calling LoadStringAsync and getting back the Task object, the remainder of the function is hooked up as a callback that will be posted to the current SynchronizationContext in order to continue execution on the right thread when the loading is complete. The compiler is layering on some really helpful syntactic sugar here. Now things get interesting. Let’s imagine LoadStringAsync is implemented as follows: 123456static async Task&lt;string&gt; LoadStringAsync()&#123; string firstName = await GetFirstNameAsync(); string lastName = await GetLastNameAsync(); return firstName + ” ” + lastName;&#125; LoadStringAsync is implemented to first asynchronously retrieve a first name, then asynchronously retrieve a last name, and then return the concatenation of the two. Notice that it’s using “await”, which, as pointed out previously, is similar to the aforementioned TPL code that uses a continuation to post back to the synchronization context that was current when the await was issued. So, here’s the crucial point: for LoadStringAsync to complete (i.e. for it to have loaded all of its data and returned its concatenated string, completing the task it returned with that concatenated result), the delegates it posted to the UI thread must have completed. If the UI thread is unable to get back to the message loop to process messages, it will be unable to pick up the posted delegates that resulted from the asynchronous operations in LoadStringAsync completing, which means the remainder of LoadStringAsync will not run, which means the Task returned from LoadStringAsync will not complete. It won’t complete until the relevant messages are processed by the message loop. With that in mind, consider this (faulty) reimplementation of button1_Click: 12345private void button1_Click(object sender, RoutedEventArgs e)&#123; Task&lt;string&gt; s = LoadStringAsync(); textBox1.Text = s.Result; // warning: buggy&#125; There’s an exceedingly good chance that this code will hang your application. The Task.Result property is strongly typed as a String, and thus it can’t return until it has the valid result string to hand back; in other words, it blocks until the result is available. We’re inside of button1_Click then blocking for LoadStringAsync to complete, but LoadStringAsync’s implementation depends on being able to post code asynchronously back to the UI to be executed, and the task returned from LoadStringAsync won’t complete until it does. LoadStringAsync is waiting for button1_Click to complete, and button1_Click is waiting for LoadStringAsync to complete. Deadlock! This problem can be exemplified easily without using any of this complicated machinery, e.g.: 123456private void button1_Click(object sender, RoutedEventArgs e)&#123; var mre = new ManualResetEvent(false); SynchronizationContext.Current.Post(_ =&gt; mre.Set(), null); mre.WaitOne(); // warning: buggy&#125; Here, we’re creating a ManualResetEvent, a synchronization primitive that allows us to synchronously wait (block) until the primitive is set. After creating it, we post back to the UI thread to set the event, and then we wait for it to be set. But we’re waiting on the very thread that would go back to the message loop to pick up the posted message to do the set operation. Deadlock. The moral of this (longer than intended) story is that you should not block the UI thread. Contrary to Nike’s recommendations, just don’t do it. The new async language functionality makes it easy to asynchronous wait for your work to complete. So, on your UI thread, instead of writing: 12Task&lt;string&gt; s = LoadStringAsync();textBox1.Text = s.Result; // BAD ON UI you can write: 12Task&lt;string&gt; s = LoadStringAsync();textBox1.Text = await s; // GOOD ON UI Or instead of writing: 12Task t = DoWork();t.Wait(); // BAD ON UI you can write: 12Task t = DoWork();await t; // GOOD ON UI This isn’t to say you should never block. To the contrary, synchronously waiting for a task to complete can be a very effective mechanism, and can exhibit less overhead in many situations than the asynchronous counterpart. There are also some contexts where asynchronously waiting can be dangerous. For these reasons and others, Task and Task support both approaches, so you can have your cake and eat it too. Just be cognizant of what you’re doing and when, and don’t block your UI thread. (One final note: the Async CTP includes the TaskEx.ConfigureAwait method. You can use this method to suppress the default behavior of marshaling back to the original synchronization context. This could have been used, for example, in the LoadStringAsync method to prevent those awaits from needing to return to the UI thread. This would not only have prevented the deadlock, it would have also resulted in better performance, because we now no longer need to force execution back to the UI thread, when nothing in that method actually needed to run on the UI thread.)","categories":[],"tags":[{"name":".NET","slug":"NET","permalink":"https://pinopino.github.io/tags/NET/"},{"name":"珠玑","slug":"珠玑","permalink":"https://pinopino.github.io/tags/%E7%8F%A0%E7%8E%91/"},{"name":"异步","slug":"异步","permalink":"https://pinopino.github.io/tags/%E5%BC%82%E6%AD%A5/"}]}],"categories":[],"tags":[{"name":".NET","slug":"NET","permalink":"https://pinopino.github.io/tags/NET/"},{"name":"异步","slug":"异步","permalink":"https://pinopino.github.io/tags/%E5%BC%82%E6%AD%A5/"},{"name":"performance","slug":"performance","permalink":"https://pinopino.github.io/tags/performance/"},{"name":"珠玑","slug":"珠玑","permalink":"https://pinopino.github.io/tags/%E7%8F%A0%E7%8E%91/"}]}